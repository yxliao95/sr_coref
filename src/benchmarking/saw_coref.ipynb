{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer_path = \"/root/workspace/fast-coref/coref_resources/reference-coreference-scorers/scorer.pl\"\n",
    "test_conll_path = \"/root/workspace/sr_coref/src/benchmarking/data/radcoref_test.conll\"\n",
    "\n",
    "output_file_path = \"/root/workspace/sr_coref/src/benchmarking/data/radcoref_test_pred.conll\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_jsonlines_path = \"/root/workspace/sr_coref/src/benchmarking/data/radcoref_input.jsonlines\"\n",
    "pred_jsonlines_path = \"/root/workspace/sr_coref/src/benchmarking/data/radcoref_pred.jsonlines\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_custom_conll(row):\n",
    "    obj = re.match(r\".+\\t\\d+\\t\\d+\\t(.*?)(\\t_){8}(\\t(.+))?\", row)\n",
    "    token_str = obj.group(1)\n",
    "    token_coref_ids = obj.group(4)\n",
    "    return token_str, token_coref_ids\n",
    "\n",
    "def extract_onto_conll(row):\n",
    "    str_list = re.split(r\" +\", row)\n",
    "    token_str = str_list[3]\n",
    "    token_coref_ids = str_list[-1] if str_list[-1] != \"-\" else None\n",
    "    return token_str, token_coref_ids\n",
    "\n",
    "row_info_extractor = extract_custom_conll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load radcoref test conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "with open(test_conll_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    rows = f.readlines()\n",
    "    rows = [i.strip(\"\\n\") for i in rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConllDocument:\n",
    "    def __init__(self, doc_key):\n",
    "        self.doc_key = doc_key\n",
    "        self.sent_toks = []\n",
    "        self.sent_tok_idx = []\n",
    "        self.gt_clusters = []  # [[start,end], ...]\n",
    "        self.pred_clusters = []\n",
    "\n",
    "        self._new_sent = True\n",
    "        self._tok_pointer = 0\n",
    "\n",
    "    def add_token(self, token):\n",
    "        if self._new_sent:\n",
    "            self.sent_toks.append([])\n",
    "            self.sent_tok_idx.append([])\n",
    "            self._new_sent = False\n",
    "        self.sent_toks[-1].append(token)\n",
    "        self.sent_tok_idx[-1].append(self._tok_pointer)\n",
    "        self._tok_pointer += 1\n",
    "        return self._tok_pointer - 1\n",
    "    \n",
    "    def add_gt_cluster(self, token_coref_id, span_start, span_end):\n",
    "        while len(self.gt_clusters) < (token_coref_id + 1):\n",
    "            self.gt_clusters.append([])\n",
    "        if span_start is not None:\n",
    "            self.gt_clusters[token_coref_id].append([span_start, span_end])\n",
    "        elif span_start == None:\n",
    "            last_none_ele = next(filter(lambda x: x[1] is None, reversed(self.gt_clusters[token_coref_id])), None)\n",
    "            assert last_none_ele is not None\n",
    "            last_none_ele[1] = span_end\n",
    "        else:\n",
    "            raise RuntimeError(\"Should not see this.\")\n",
    "    \n",
    "    def add_pred_cluster(self, coref_id, span_start, span_end):\n",
    "        while len(self.pred_clusters) < (coref_id + 1):\n",
    "            self.pred_clusters.append([])\n",
    "        self.pred_clusters[coref_id].append([span_start, span_end])\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"{self.doc_key}: {self.gt_clusters}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_doc_obj = None\n",
    "doc_objs = []\n",
    "for row in rows:\n",
    "    if row == \"\" and current_doc_obj == None:\n",
    "        continue\n",
    "\n",
    "    if row.startswith(\"#begin\"):\n",
    "        obj = re.match(r\"#begin document \\((.+)\\); part 0\", row)\n",
    "        dockey = obj.group(1)\n",
    "        current_doc_obj = ConllDocument(dockey)\n",
    "    elif row == \"#end document\":\n",
    "        doc_objs.append(current_doc_obj)\n",
    "        current_doc_obj = None\n",
    "    else:\n",
    "        assert current_doc_obj != None\n",
    "        \n",
    "        # next sentence identifier\n",
    "        if row == \"\":\n",
    "            current_doc_obj._new_sent = True\n",
    "            continue\n",
    "\n",
    "        token_str, token_coref_ids= row_info_extractor(row)\n",
    "\n",
    "        # extracted token str\n",
    "        tok_idx = current_doc_obj.add_token(token_str)\n",
    "\n",
    "        # identify the coref cluster to which the token belongs\n",
    "        if token_coref_ids:\n",
    "            token_coref_id_list = token_coref_ids.split(\"|\")\n",
    "            for token_coref_id_str in token_coref_id_list:\n",
    "                token_coref_id = int(token_coref_id_str.strip(\"()\"))\n",
    "                span_start = tok_idx if token_coref_id_str.startswith(\"(\") else None\n",
    "                span_end = tok_idx if token_coref_id_str.endswith(\")\") else None\n",
    "                current_doc_obj.add_gt_cluster(token_coref_id, span_start, span_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build input for model\n",
    "\n",
    "See https://github.com/kareldo/wl-coref for more details\n",
    "\n",
    "To predict coreference relations on an arbitrary text, you will need to prepare the data in the jsonlines format (one json-formatted document per line). The following fields are requred:\n",
    "{\n",
    "    \"document_id\": \"tc_mydoc_001\",\n",
    "    \"cased_words\": [\"Hi\", \"!\", \"Bye\", \".\"],\n",
    "    \"sent_id\": [0, 0, 1, 1]\n",
    "}\n",
    "\n",
    "document_id can be any string that starts with a two-letter genre identifier. The genres recognized are the following:\n",
    "\n",
    "bc: broadcast conversation\n",
    "bn: broadcast news\n",
    "mz: magazine genre (Sinorama magazine)\n",
    "nw: newswire genre\n",
    "pt: pivot text (The Bible)\n",
    "tc: telephone conversation (CallHome corpus)\n",
    "wb: web data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_obj in doc_objs:\n",
    "    doc_id = f\"wb_{doc_obj.doc_key}\"\n",
    "    cased_words = []\n",
    "    sentences_map = []\n",
    "    for sent_id, sent in enumerate(doc_obj.sent_toks):\n",
    "        for tok in sent:\n",
    "            cased_words.append(tok)\n",
    "            sentences_map.append(sent_id)\n",
    "    out = {\n",
    "        \"document_id\": doc_id,\n",
    "        \"cased_words\": cased_words,\n",
    "        \"sent_id\": sentences_map,\n",
    "    }\n",
    "    with open(input_jsonlines_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(out))\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using caw-coref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /root/autodl-tmp/hg_offline_models/roberta-large...\n",
      "Using tokenizer kwargs: {'add_prefix_space': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /root/autodl-tmp/hg_offline_models/roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Bert successfully loaded.\n",
      "Loading from /root/autodl-tmp/hg_offline_models/caw-coref/roberta_release.pt...\n",
      "Loaded bert\n",
      "Loaded we\n",
      "Loaded rough_scorer\n",
      "Loaded pw\n",
      "Loaded a_scorer\n",
      "Loaded sp\n",
      "100%|███████████████████████████████████████| 200/200 [00:08<00:00, 24.49docs/s]\n"
     ]
    }
   ],
   "source": [
    "!python \\\n",
    "    /root/workspace/sr_coref/src/benchmarking/wl-coref/predict.py \\\n",
    "    roberta \\\n",
    "    $input_jsonlines_path \\\n",
    "    $pred_jsonlines_path \\\n",
    "    --weights /root/autodl-tmp/hg_offline_models/caw-coref/roberta_release.pt \\\n",
    "    --config-file /root/workspace/sr_coref/src/benchmarking/wl-coref/config.toml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output pred conll file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConllToken(object):\n",
    "    def __init__(self, docId, sentenceId, tokenId, tokenStr):\n",
    "        self.docId = docId\n",
    "        self.sentenceId = sentenceId\n",
    "        self.tokenId = tokenId\n",
    "        self.tokenStr = tokenStr\n",
    "        self.corefLabel = \"\"\n",
    "\n",
    "    def add_coref_label(self, label, label_type):\n",
    "        if label_type == \"start\":\n",
    "            label = f\"({label}\"\n",
    "        elif label_type == \"end\":\n",
    "            label = f\"{label})\"\n",
    "        elif label_type == \"both\":\n",
    "            label = f\"({label})\"\n",
    "            \n",
    "        if not self.corefLabel:\n",
    "            self.corefLabel = label\n",
    "        else:\n",
    "            self.corefLabel = f\"{self.corefLabel}|{label}\"\n",
    "\n",
    "    def get_conll_str(self):\n",
    "        # IMPORTANT! Any tokens that trigger regex: \\((\\d+) or (\\d+)\\) will also\n",
    "        # trigger \"conll/reference-coreference-scorers\" unexpectedly,\n",
    "        # which will either cause execution error or wrong metric score.\n",
    "        # See coref/wrong_conll_scorer_example for details.\n",
    "        tok_str = self.tokenStr\n",
    "        if re.search(r\"\\(?[^A-Za-z]+\\)?\", tok_str):\n",
    "            tok_str = tok_str.replace(\"(\", \"[\").replace(\")\", \"]\")\n",
    "        if tok_str.strip() == \"\":\n",
    "            tok_str = \"\"\n",
    "        if self.corefLabel:\n",
    "            return f\"{self.docId}\\t0\\t{self.tokenId}\\t{tok_str}\\t\" + \"_\\t\" * 8 + self.corefLabel\n",
    "        return f\"{self.docId}\\t0\\t{self.tokenId}\\t{tok_str}\\t\" + \"_\\t\" * 7 + \"_\"\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"{self.tokenStr}({self.sentenceId}:{self.tokenId})|[{self.corefLabel}]\"\n",
    "\n",
    "    __repr__ = __str__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pred_jsonlines_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    pred_docs = f.readlines()\n",
    "\n",
    "pred_doc_dict = {}\n",
    "for pred_doc in pred_docs:\n",
    "    doc = json.loads(pred_doc)\n",
    "    doc_key = doc[\"document_id\"].lstrip(\"wb_\")\n",
    "    pred_doc_dict[doc_key] = doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_obj in doc_objs:\n",
    "    BEGIN = f\"#begin document ({doc_obj.doc_key}); part 0\\n\"\n",
    "    SENTENCE_SEPARATOR = \"\\n\"\n",
    "    END = \"#end document\\n\"\n",
    "    \n",
    "    sentence_list = []\n",
    "    for sent_id, sent in enumerate(doc_obj.sent_toks):\n",
    "        token_list = []\n",
    "        for tok_id, tok in enumerate(sent):\n",
    "            conll_token = ConllToken(docId=doc_obj.doc_key, \n",
    "                                    sentenceId=sent_id,\n",
    "                                    tokenId=tok_id, \n",
    "                                    tokenStr=tok)\n",
    "            token_list.append(conll_token)\n",
    "        sentence_list.append(token_list)\n",
    "        \n",
    "    conll_tokens = [c_tok for sent in sentence_list for c_tok in sent]\n",
    "    for coref_id, cluster in enumerate(pred_doc_dict[doc_obj.doc_key][\"span_clusters\"]):\n",
    "        for span in cluster:\n",
    "            start_idx = span[0]\n",
    "            end_idx = span[1]-1\n",
    "            if start_idx == end_idx:\n",
    "                conll_tokens[start_idx].add_coref_label(coref_id, label_type=\"both\")\n",
    "            else:\n",
    "                conll_tokens[start_idx].add_coref_label(coref_id, label_type=\"start\")\n",
    "                conll_tokens[end_idx].add_coref_label(coref_id, label_type=\"end\")\n",
    "    \n",
    "    with open(output_file_path, \"a\", encoding=\"UTF-8\") as out:\n",
    "        out.write(BEGIN)\n",
    "        for sent in sentence_list:\n",
    "            for tok in sent:\n",
    "                out.write(tok.get_conll_str() + \"\\n\")\n",
    "            out.write(SENTENCE_SEPARATOR)\n",
    "        out.write(END)\n",
    "        out.write(SENTENCE_SEPARATOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from subprocess import PIPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_conll_script(\n",
    "    scorer_path: str, use_which_metric: str, groundtruth_file_path: str, predicted_file_path: str\n",
    "):\n",
    "    \"\"\"Args:\n",
    "        scorer_path: The path of the CoNLL scorer script: scorer.pl\n",
    "        use_which_metric: muc, bclub, ceafe\n",
    "        groundtruth_file_path: The path of the file serve as a ground truth file\n",
    "        predicted_file_path: The path of the file serve as a predicted output\n",
    "\n",
    "    Returns:\n",
    "        out: The standard output of the script.\n",
    "        err: The error message if the script is failed. Empty if no error.\n",
    "    \"\"\"\n",
    "    command = [scorer_path, use_which_metric, groundtruth_file_path, predicted_file_path, \"none\"]\n",
    "\n",
    "    result = subprocess.run(command, stdout=PIPE, stderr=PIPE)\n",
    "    out = result.stdout.decode(\"utf-8\")\n",
    "    err = result.stderr.decode(\"utf-8\")\n",
    "    if err:\n",
    "        err += f\" Error command: {command}\"\n",
    "    return out, err\n",
    "\n",
    "def resolve_conll_script_output(output_str):\n",
    "    \"\"\"Args:\n",
    "        output_str: The output of the CoNLL scorer script: scorer.pl. It only support single metric output, i.e. muc, bcub, ceafe, ceafm\n",
    "    Returns:\n",
    "        The percentage float value extracted from the script output. The ``%`` symble is omitted.\n",
    "    \"\"\"\n",
    "    regexPattern = r\"(\\d*\\.?\\d*)%\"\n",
    "    scores = [float(i) for i in re.findall(regexPattern, output_str)]\n",
    "    mention_recall = scores[0]\n",
    "    mention_precision = scores[1]\n",
    "    mention_f1 = scores[2]\n",
    "    coref_recall = scores[3]\n",
    "    coref_precision = scores[4]\n",
    "    coref_f1 = scores[5]\n",
    "    return mention_recall, mention_precision, mention_f1, coref_recall, coref_precision, coref_f1\n",
    "\n",
    "def compute_conll_score(conll_file_gt, conll_file_pred):\n",
    "    print(\"gt:\", conll_file_gt)\n",
    "    print(\"pred:\", conll_file_pred)\n",
    "    overall_f1 = []\n",
    "    for metric in ['muc', 'bcub', 'ceafe']:\n",
    "        out, err = invoke_conll_script(scorer_path, metric, conll_file_gt, conll_file_pred)\n",
    "        mention_recall, mention_precision, mention_f1, coref_recall, coref_precision, coref_f1 = resolve_conll_script_output(out)\n",
    "        overall_f1.append(coref_f1)\n",
    "        print(f\"Metric: {metric}\")\n",
    "        print(f\"mention_recall, mention_precision, mention_f1: {mention_recall}, {mention_precision}, {mention_f1}\")\n",
    "        print(f\"coref_recall, coref_precision, coref_f1: {coref_recall}, {coref_precision}, {coref_f1}\")\n",
    "\n",
    "    print(f\"Overall F1: {sum(overall_f1) / len(overall_f1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt: /root/workspace/sr_coref/src/benchmarking/data/radcoref_test.conll\n",
      "pred: /root/workspace/sr_coref/src/benchmarking/data/radcoref_test_pred.conll\n",
      "Metric: muc\n",
      "mention_recall, mention_precision, mention_f1: 66.21, 82.94, 73.64\n",
      "coref_recall, coref_precision, coref_f1: 59.19, 74.8, 66.08\n",
      "Metric: bcub\n",
      "mention_recall, mention_precision, mention_f1: 66.21, 82.94, 73.64\n",
      "coref_recall, coref_precision, coref_f1: 61.72, 77.73, 68.81\n",
      "Metric: ceafe\n",
      "mention_recall, mention_precision, mention_f1: 66.21, 82.94, 73.64\n",
      "coref_recall, coref_precision, coref_f1: 65.51, 81.22, 72.52\n",
      "Overall F1: 69.13666666666666\n"
     ]
    }
   ],
   "source": [
    "compute_conll_score(conll_file_gt=test_conll_path, conll_file_pred=output_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coref_benchmark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
