{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "scorer_path = \"/root/workspace/fast-coref/coref_resources/reference-coreference-scorers/scorer.pl\"\n",
    "\n",
    "test_conll_path = \"/root/workspace/sr_coref/src/benchmarking/data/radcoref_test.conll\"\n",
    "test_jsonlines_path = \"/root/workspace/sr_coref/src/benchmarking/data/radcoref_test.jsonlines\"\n",
    "\n",
    "output_file_path = \"/root/workspace/sr_coref/src/benchmarking/data/radcoref_pred_test.conll\"\n",
    "\n",
    "corefqa_repo_path = \"/root/workspace/sr_coref/src/benchmarking/CorefQA/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(corefqa_repo_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert radcoref test conll to jsonlines for CorefQA\n",
    "\n",
    "In jsonlines, the `clusters` are represented as subtok_idx. To get the doctok_idx, we should map by using `subtoken_map`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import collections\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import conll\n",
    "import util\n",
    "from bert import tokenization\n",
    "\n",
    "vocab_file_path = os.path.join(corefqa_repo_path, \"cased_config_vocab\", \"vocab.txt\")\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file=vocab_file_path, do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentState(object):\n",
    "    def __init__(self, key):\n",
    "        self.doc_key = key\n",
    "        self.sentence_end = []\n",
    "        self.token_end = []\n",
    "        self.tokens = []\n",
    "        self.subtokens = []\n",
    "        self.info = []\n",
    "        self.segments = []\n",
    "        self.subtoken_map = []\n",
    "        self.segment_subtoken_map = []\n",
    "        self.sentence_map = []\n",
    "        self.pronouns = []\n",
    "        self.clusters = collections.defaultdict(list)\n",
    "        self.coref_stacks = collections.defaultdict(list)\n",
    "        self.speakers = []\n",
    "        self.segment_info = []\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"tokens: {self.tokens}\"\n",
    "    \n",
    "    def finalize(self):\n",
    "        # finalized: segments, segment_subtoken_map\n",
    "        # populate speakers from info\n",
    "        subtoken_idx = 0\n",
    "        for segment in self.segment_info:\n",
    "            speakers = []\n",
    "            for i, tok_info in enumerate(segment):\n",
    "                if tok_info is None and (i == 0 or i == len(segment) - 1):\n",
    "                    speakers.append(\"[SPL]\")\n",
    "                elif tok_info is None:\n",
    "                    speakers.append(speakers[-1])\n",
    "                else:\n",
    "                    speakers.append(tok_info[9])\n",
    "                    if tok_info[4] == \"PRP\":\n",
    "                        self.pronouns.append(subtoken_idx)\n",
    "                subtoken_idx += 1\n",
    "            self.speakers += [speakers]\n",
    "        # populate sentence map\n",
    "\n",
    "        # populate clusters\n",
    "        first_subtoken_index = -1\n",
    "        for seg_idx, segment in enumerate(self.segment_info):\n",
    "            speakers = []\n",
    "            for i, tok_info in enumerate(segment):\n",
    "                first_subtoken_index += 1\n",
    "                coref = tok_info[-2] if tok_info is not None else \"-\"\n",
    "                if coref != \"-\":\n",
    "                    last_subtoken_index = first_subtoken_index + \\\n",
    "                        tok_info[-1] - 1\n",
    "                    for part in coref.split(\"|\"):\n",
    "                        if part[0] == \"(\":\n",
    "                            if part[-1] == \")\":\n",
    "                                cluster_id = int(part[1:-1])\n",
    "                                self.clusters[cluster_id].append(\n",
    "                                    (first_subtoken_index, last_subtoken_index))\n",
    "                            else:\n",
    "                                cluster_id = int(part[1:])\n",
    "                                self.coref_stacks[cluster_id].append(\n",
    "                                    first_subtoken_index)\n",
    "                        else:\n",
    "                            cluster_id = int(part[:-1])\n",
    "                            start = self.coref_stacks[cluster_id].pop()\n",
    "                            self.clusters[cluster_id].append(\n",
    "                                (start, last_subtoken_index))\n",
    "        # merge clusters\n",
    "        merged_clusters = []\n",
    "        for c1 in self.clusters.values():\n",
    "            existing = None\n",
    "            for m in c1:\n",
    "                for c2 in merged_clusters:\n",
    "                    if m in c2:\n",
    "                        existing = c2\n",
    "                        break\n",
    "                if existing is not None:\n",
    "                    break\n",
    "            if existing is not None:\n",
    "                print(\"Merging clusters (shouldn't happen very often.)\")\n",
    "                existing.update(c1)\n",
    "            else:\n",
    "                merged_clusters.append(set(c1))\n",
    "        merged_clusters = [list(c) for c in merged_clusters]\n",
    "        all_mentions = util.flatten(merged_clusters)\n",
    "        sentence_map = get_sentence_map(self.segments, self.sentence_end)\n",
    "        subtoken_map = util.flatten(self.segment_subtoken_map)\n",
    "        assert len(all_mentions) == len(set(all_mentions))\n",
    "        num_words = len(util.flatten(self.segments))\n",
    "        assert num_words == len(util.flatten(self.speakers))\n",
    "        assert num_words == len(subtoken_map), (num_words, len(subtoken_map))\n",
    "        assert num_words == len(sentence_map), (num_words, len(sentence_map))\n",
    "        return {\"doc_key\": self.doc_key, \"sentences\": self.segments, \"speakers\": self.speakers, \"constituents\": [], \"ner\": [], \"clusters\": merged_clusters, \"sentence_map\": sentence_map, \"subtoken_map\": subtoken_map, \"pronouns\": self.pronouns}\n",
    "    \n",
    "def get_sentence_map(segments, sentence_end):\n",
    "    current = 0\n",
    "    sent_map = []\n",
    "    sent_end_idx = 0\n",
    "    assert len(sentence_end) == sum([len(s) - 2 for s in segments])\n",
    "    for segment in segments:\n",
    "        sent_map.append(current)\n",
    "        for i in range(len(segment) - 2):\n",
    "            sent_map.append(current)\n",
    "            current += int(sentence_end[sent_end_idx])\n",
    "            sent_end_idx += 1\n",
    "        sent_map.append(current)\n",
    "    return sent_map\n",
    "\n",
    "\n",
    "def normalize_word(word):\n",
    "    if word == \"/.\" or word == \"/?\":\n",
    "        return word[1:]\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "# first try to satisfy constraints1, and if not possible, constraints2.\n",
    "def split_into_segments(document_state, max_segment_len, constraints1, constraints2):\n",
    "    current = 0\n",
    "    previous_token = 0\n",
    "    while current < len(document_state.subtokens):\n",
    "        end = min(current + max_segment_len - 1 - 2, len(document_state.subtokens) - 1)\n",
    "        while end >= current and not constraints1[end]:\n",
    "            end -= 1\n",
    "        if end < current:\n",
    "            end = min(current + max_segment_len - 1 - 2, len(document_state.subtokens) - 1)\n",
    "            while end >= current and not constraints2[end]:\n",
    "                end -= 1\n",
    "            if end < current:\n",
    "                raise Exception(\"Can't find valid segment\")\n",
    "        document_state.segments.append([\"[CLS]\"] + document_state.subtokens[current : end + 1] + [\"[SEP]\"])\n",
    "        subtoken_map = document_state.subtoken_map[current : end + 1]\n",
    "        document_state.segment_subtoken_map.append([previous_token] + subtoken_map + [subtoken_map[-1]])\n",
    "        info = document_state.info[current : end + 1]\n",
    "        document_state.segment_info.append([None] + info + [None])\n",
    "        current = end + 1\n",
    "        previous_token = subtoken_map[-1]\n",
    "\n",
    "def get_document(document_lines, tokenizer, segment_len):\n",
    "    document_state = DocumentState(document_lines[0])\n",
    "    word_idx = -1\n",
    "    for line in document_lines[1]:\n",
    "        row = line.split()\n",
    "        if len(row) == 12:\n",
    "            row.append(\"-\") # follow the same structure as ontonotes conll files ()\n",
    "        sentence_end = len(row) == 0\n",
    "        if not sentence_end:\n",
    "            assert len(row) == 13\n",
    "            word_idx += 1\n",
    "            word = normalize_word(row[3])\n",
    "            subtokens = tokenizer.tokenize(word)\n",
    "            document_state.tokens.append(word)\n",
    "            document_state.token_end += ([False] * (len(subtokens) - 1)) + [True]\n",
    "            for sidx, subtoken in enumerate(subtokens):\n",
    "                document_state.subtokens.append(subtoken)\n",
    "                info = None if sidx != 0 else (row + [len(subtokens)])\n",
    "                document_state.info.append(info)\n",
    "                document_state.sentence_end.append(False)\n",
    "                document_state.subtoken_map.append(word_idx)\n",
    "        else:\n",
    "            document_state.sentence_end[-1] = True\n",
    "    split_into_segments(document_state, segment_len, document_state.sentence_end, document_state.token_end)\n",
    "    document = document_state.finalize()\n",
    "    return document\n",
    "\n",
    "\n",
    "def minimize_partition(tokenizer, input_path, output_path, seg_len=512):\n",
    "    count = 0\n",
    "    print(\"Minimizing {}\".format(input_path))\n",
    "    documents = []\n",
    "    with open(input_path, \"r\") as input_file:\n",
    "        for line in input_file.readlines():\n",
    "            begin_document_match = re.match(conll.BEGIN_DOCUMENT_REGEX, line)\n",
    "            if begin_document_match:\n",
    "                doc_key = conll.get_doc_key(begin_document_match.group(1), begin_document_match.group(2))\n",
    "                documents.append((doc_key, []))\n",
    "            elif line.startswith(\"#end document\"):\n",
    "                continue\n",
    "            else:\n",
    "                documents[-1][1].append(line)\n",
    "    with open(output_path, \"w\") as output_file:\n",
    "        for document_lines in documents:\n",
    "            document = get_document(document_lines, tokenizer, seg_len)\n",
    "            output_file.write(json.dumps(document))\n",
    "            output_file.write(\"\\n\")\n",
    "            count += 1\n",
    "    print(\"Wrote {} documents to {}\".format(count, output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimizing /root/workspace/sr_coref/src/benchmarking/data/radcoref_test.conll\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 200 documents to /root/workspace/sr_coref/src/benchmarking/data/radcoref_test.jsonlines\n"
     ]
    }
   ],
   "source": [
    "minimize_partition(tokenizer, input_path=test_conll_path, output_path=test_jsonlines_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = [[\"[CLS]\", \"As\", \"compared\", \"to\", \"the\", \"previous\", \"image\", \",\", \"the\", \"alignment\", \"of\", \"the\", \"stern\", \"##al\", \"wires\", \"is\", \"unchanged\", \".\", \"Un\", \"##chang\", \"##ed\", \"position\", \"of\", \"the\", \"right\", \"internal\", \"j\", \"##ug\", \"##ular\", \"vein\", \"cat\", \"##he\", \"##ter\", \",\", \"with\", \"its\", \"tip\", \"projecting\", \"over\", \"the\", \"mid\", \"to\", \"lower\", \"SV\", \"##C\", \".\", \"No\", \"p\", \"##ne\", \"##um\", \"##oth\", \"##orax\", \".\", \"Small\", \"bilateral\", \"p\", \"##le\", \"##ural\", \"e\", \"##ff\", \"##usions\", \"are\", \"better\", \"appreciated\", \"on\", \"the\", \"lateral\", \"than\", \"on\", \"the\", \"frontal\", \"image\", \".\", \"There\", \"are\", \"limited\", \"to\", \"the\", \"cost\", \"##op\", \"##hren\", \"##ic\", \"sin\", \"##uses\", \".\", \"No\", \"pulmonary\", \"ed\", \"##ema\", \".\", \"No\", \"pneumonia\", \".\", \"[SEP]\"]]\n",
    "\n",
    "subtok_map = [0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 11, 12, 13, 14, 15, 16, 16, 16, 17, 18, 19, 20, 21, 22, 22, 22, 23, 24, 24, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 35, 36, 37, 38, 38, 38, 38, 38, 39, 40, 41, 42, 42, 42, 43, 43, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 61, 61, 61, 62, 62, 63, 64, 65, 66, 66, 67, 68, 69, 70, 70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[27]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = [i for sent in sents for i in sent]\n",
    "\n",
    "subtok_map[35:35+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "s52428114_impression: [[[19, 24], [27, 27]]]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_objs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load radcoref test conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "with open(test_conll_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    rows = f.readlines()\n",
    "    rows = [i.strip(\"\\n\") for i in rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_onto_conll(row):\n",
    "    str_list = re.split(r\" +\", row)\n",
    "    token_str = str_list[3]\n",
    "    token_coref_ids = str_list[-1] if str_list[-1] != \"-\" else None\n",
    "    return token_str, token_coref_ids\n",
    "\n",
    "row_info_extractor = extract_custom_conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConllDocument:\n",
    "    def __init__(self, doc_key):\n",
    "        self.doc_key = doc_key\n",
    "        self.sent_toks = []\n",
    "        self.sent_tok_idx = []\n",
    "        self.gt_clusters = []  # [[start,end], ...]\n",
    "        self.pred_clusters = []\n",
    "\n",
    "        self._new_sent = True\n",
    "        self._tok_pointer = 0\n",
    "\n",
    "    def add_token(self, token):\n",
    "        if self._new_sent:\n",
    "            self.sent_toks.append([])\n",
    "            self.sent_tok_idx.append([])\n",
    "            self._new_sent = False\n",
    "        self.sent_toks[-1].append(token)\n",
    "        self.sent_tok_idx[-1].append(self._tok_pointer)\n",
    "        self._tok_pointer += 1\n",
    "        return self._tok_pointer - 1\n",
    "    \n",
    "    def add_gt_cluster(self, token_coref_id, span_start, span_end):\n",
    "        while len(self.gt_clusters) < (token_coref_id + 1):\n",
    "            self.gt_clusters.append([])\n",
    "        if span_start is not None:\n",
    "            self.gt_clusters[token_coref_id].append([span_start, span_end])\n",
    "        elif span_start == None:\n",
    "            last_none_ele = next(filter(lambda x: x[1] is None, reversed(self.gt_clusters[token_coref_id])), None)\n",
    "            assert last_none_ele is not None\n",
    "            last_none_ele[1] = span_end\n",
    "        else:\n",
    "            raise RuntimeError(\"Should not see this.\")\n",
    "    \n",
    "    def add_pred_cluster(self, coref_id, span_start, span_end):\n",
    "        while len(self.pred_clusters) < (coref_id + 1):\n",
    "            self.pred_clusters.append([])\n",
    "        self.pred_clusters[coref_id].append([span_start, span_end])\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"{self.doc_key}: {self.gt_clusters}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_doc_obj = None\n",
    "doc_objs = []\n",
    "for row in rows:\n",
    "    if row == \"\" and current_doc_obj == None:\n",
    "        continue\n",
    "\n",
    "    if row.startswith(\"#begin\"):\n",
    "        obj = re.match(r\"#begin document \\((.+)\\); part 0\", row)\n",
    "        dockey = obj.group(1)\n",
    "        current_doc_obj = ConllDocument(dockey)\n",
    "    elif row == \"#end document\":\n",
    "        doc_objs.append(current_doc_obj)\n",
    "        current_doc_obj = None\n",
    "    else:\n",
    "        assert current_doc_obj != None\n",
    "        \n",
    "        # next sentence identifier\n",
    "        if row == \"\":\n",
    "            current_doc_obj._new_sent = True\n",
    "            continue\n",
    "\n",
    "        token_str, token_coref_ids= row_info_extractor(row)\n",
    "\n",
    "        # extracted token str\n",
    "        tok_idx = current_doc_obj.add_token(token_str)\n",
    "\n",
    "        # identify the coref cluster to which the token belongs\n",
    "        if token_coref_ids:\n",
    "            token_coref_id_list = token_coref_ids.split(\"|\")\n",
    "            for token_coref_id_str in token_coref_id_list:\n",
    "                token_coref_id = int(token_coref_id_str.strip(\"()\"))\n",
    "                span_start = tok_idx if token_coref_id_str.startswith(\"(\") else None\n",
    "                span_end = tok_idx if token_coref_id_str.endswith(\")\") else None\n",
    "                current_doc_obj.add_gt_cluster(token_coref_id, span_start, span_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU=0 python /root/workspace/sr_coref/src/benchmarking/CorefQA/predict.py spanbert_large /root/workspace/sr_coref/src/benchmarking/data/radcoref.txt /root/workspace/sr_coref/src/benchmarking/data/radcoref_out.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coref_benchmark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
