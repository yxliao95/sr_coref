{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer_path = \"/root/workspace/fast-coref/coref_resources/reference-coreference-scorers/scorer.pl\"\n",
    "test_conll_path = \"/root/workspace/sr_coref/src/benchmarking/data/radcoref_test.conll\"\n",
    "output_file_path = \"/root/workspace/sr_coref/src/benchmarking/data/radcoref_pred_test.conll\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_custom_conll(row):\n",
    "    obj = re.match(r\".+\\t\\d+\\t\\d+\\t(.*?)(\\t_){8}(\\t(.+))?\", row)\n",
    "    token_str = obj.group(1)\n",
    "    token_coref_ids = obj.group(4)\n",
    "    return token_str, token_coref_ids\n",
    "\n",
    "\n",
    "def extract_onto_conll(row):\n",
    "    str_list = re.split(r\" +\", row)\n",
    "    token_str = str_list[3]\n",
    "    token_coref_ids = str_list[-1] if str_list[-1] != \"-\" else None\n",
    "    return token_str, token_coref_ids\n",
    "\n",
    "\n",
    "row_info_extractor = extract_custom_conll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load radcoref test conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "with open(test_conll_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    rows = f.readlines()\n",
    "    rows = [i.strip(\"\\n\") for i in rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConllDocument:\n",
    "    def __init__(self, doc_key):\n",
    "        self.doc_key = doc_key\n",
    "        self.sent_toks = []\n",
    "        self.sent_tok_idx = []\n",
    "        self.gt_clusters = []  # [[start,end], ...]\n",
    "        self.pred_clusters = []\n",
    "\n",
    "        self._new_sent = True\n",
    "        self._tok_pointer = 0\n",
    "\n",
    "    def add_token(self, token):\n",
    "        if self._new_sent:\n",
    "            self.sent_toks.append([])\n",
    "            self.sent_tok_idx.append([])\n",
    "            self._new_sent = False\n",
    "        self.sent_toks[-1].append(token)\n",
    "        self.sent_tok_idx[-1].append(self._tok_pointer)\n",
    "        self._tok_pointer += 1\n",
    "        return self._tok_pointer - 1\n",
    "\n",
    "    def add_gt_cluster(self, token_coref_id, span_start, span_end):\n",
    "        while len(self.gt_clusters) < (token_coref_id + 1):\n",
    "            self.gt_clusters.append([])\n",
    "        if span_start is not None:\n",
    "            self.gt_clusters[token_coref_id].append([span_start, span_end])\n",
    "        elif span_start == None:\n",
    "            last_none_ele = next(filter(lambda x: x[1] is None, reversed(self.gt_clusters[token_coref_id])), None)\n",
    "            assert last_none_ele is not None\n",
    "            last_none_ele[1] = span_end\n",
    "        else:\n",
    "            raise RuntimeError(\"Should not see this.\")\n",
    "\n",
    "    def add_pred_cluster(self, coref_id, span_start, span_end):\n",
    "        while len(self.pred_clusters) < (coref_id + 1):\n",
    "            self.pred_clusters.append([])\n",
    "        self.pred_clusters[coref_id].append([span_start, span_end])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.doc_key}: {self.gt_clusters}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_doc_obj = None\n",
    "doc_objs = []\n",
    "for row in rows:\n",
    "    if row == \"\" and current_doc_obj == None:\n",
    "        continue\n",
    "\n",
    "    if row.startswith(\"#begin\"):\n",
    "        obj = re.match(r\"#begin document \\((.+)\\); part 0\", row)\n",
    "        dockey = obj.group(1)\n",
    "        current_doc_obj = ConllDocument(dockey)\n",
    "    elif row == \"#end document\":\n",
    "        doc_objs.append(current_doc_obj)\n",
    "        current_doc_obj = None\n",
    "    else:\n",
    "        assert current_doc_obj != None\n",
    "\n",
    "        # next sentence identifier\n",
    "        if row == \"\":\n",
    "            current_doc_obj._new_sent = True\n",
    "            continue\n",
    "\n",
    "        token_str, token_coref_ids = row_info_extractor(row)\n",
    "\n",
    "        # extracted token str\n",
    "        tok_idx = current_doc_obj.add_token(token_str)\n",
    "\n",
    "        # identify the coref cluster to which the token belongs\n",
    "        if token_coref_ids:\n",
    "            token_coref_id_list = token_coref_ids.split(\"|\")\n",
    "            for token_coref_id_str in token_coref_id_list:\n",
    "                token_coref_id = int(token_coref_id_str.strip(\"()\"))\n",
    "                span_start = tok_idx if token_coref_id_str.startswith(\"(\") else None\n",
    "                span_end = tok_idx if token_coref_id_str.endswith(\")\") else None\n",
    "                current_doc_obj.add_gt_cluster(token_coref_id, span_start, span_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using neural-coref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.tagging\n",
    "\n",
    "local_config_path = \"/root/autodl-tmp/hg_offline_models/spanbert_large_cased\"\n",
    "\n",
    "predictor = Predictor.from_path(\n",
    "    \"/root/autodl-tmp/hg_offline_models/allennlp-coref/coref-spanbert-large.tar.gz\",\n",
    "    overrides={\n",
    "        \"dataset_reader\": {\n",
    "            \"token_indexers\": {\n",
    "                \"tokens\": {\n",
    "                    \"model_name\": local_config_path\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"model\": {\n",
    "            \"text_field_embedder\": {\n",
    "                \"token_embedders\": {\n",
    "                    \"tokens\": {\n",
    "                        \"model_name\": local_config_path\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"validation_dataset_reader\": {\n",
    "            \"token_indexers\": {\n",
    "                \"tokens\": {\n",
    "                    \"model_name\": local_config_path\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_obj in doc_objs:\n",
    "    words = [tok for sent in doc_obj.sent_toks for tok in sent]\n",
    "    out = predictor.predict(document=\" \".join(words))\n",
    "\n",
    "    # process the results\n",
    "    for cluster_id, cluster in enumerate(out[\"clusters\"]):\n",
    "        for mention in cluster:\n",
    "            start_idx = mention[0]\n",
    "            end_idx = mention[1]\n",
    "            doc_obj.add_pred_cluster(cluster_id, start_idx, end_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output pred conll file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConllToken(object):\n",
    "    def __init__(self, docId, sentenceId, tokenId, tokenStr):\n",
    "        self.docId = docId\n",
    "        self.sentenceId = sentenceId\n",
    "        self.tokenId = tokenId\n",
    "        self.tokenStr = tokenStr\n",
    "        self.corefLabel = \"\"\n",
    "\n",
    "    def add_coref_label(self, label, label_type):\n",
    "        if label_type == \"start\":\n",
    "            label = f\"({label}\"\n",
    "        elif label_type == \"end\":\n",
    "            label = f\"{label})\"\n",
    "        elif label_type == \"both\":\n",
    "            label = f\"({label})\"\n",
    "\n",
    "        if not self.corefLabel:\n",
    "            self.corefLabel = label\n",
    "        else:\n",
    "            self.corefLabel = f\"{self.corefLabel}|{label}\"\n",
    "\n",
    "    def get_conll_str(self):\n",
    "        # IMPORTANT! Any tokens that trigger regex: \\((\\d+) or (\\d+)\\) will also\n",
    "        # trigger \"conll/reference-coreference-scorers\" unexpectedly,\n",
    "        # which will either cause execution error or wrong metric score.\n",
    "        # See coref/wrong_conll_scorer_example for details.\n",
    "        tok_str = self.tokenStr\n",
    "        if re.search(r\"\\(?[^A-Za-z]+\\)?\", tok_str):\n",
    "            tok_str = tok_str.replace(\"(\", \"[\").replace(\")\", \"]\")\n",
    "        if tok_str.strip() == \"\":\n",
    "            tok_str = \"\"\n",
    "        if self.corefLabel:\n",
    "            return f\"{self.docId}\\t0\\t{self.tokenId}\\t{tok_str}\\t\" + \"_\\t\" * 8 + self.corefLabel\n",
    "        return f\"{self.docId}\\t0\\t{self.tokenId}\\t{tok_str}\\t\" + \"_\\t\" * 7 + \"_\"\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"{self.tokenStr}({self.sentenceId}:{self.tokenId})|[{self.corefLabel}]\"\n",
    "\n",
    "    __repr__ = __str__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_obj in doc_objs:\n",
    "    BEGIN = f\"#begin document ({doc_obj.doc_key}); part 0\\n\"\n",
    "    SENTENCE_SEPARATOR = \"\\n\"\n",
    "    END = \"#end document\\n\"\n",
    "\n",
    "    sentence_list = []\n",
    "    for sent_id, sent in enumerate(doc_obj.sent_toks):\n",
    "        token_list = []\n",
    "        for tok_id, tok in enumerate(sent):\n",
    "            conll_token = ConllToken(docId=doc_obj.doc_key, sentenceId=sent_id, tokenId=tok_id, tokenStr=tok)\n",
    "            token_list.append(conll_token)\n",
    "        sentence_list.append(token_list)\n",
    "\n",
    "    conll_tokens = [c_tok for sent in sentence_list for c_tok in sent]\n",
    "    for coref_id, cluster in enumerate(doc_obj.pred_clusters):\n",
    "        for span in cluster:\n",
    "            start_idx = span[0]\n",
    "            end_idx = span[1]\n",
    "            if start_idx == end_idx:\n",
    "                conll_tokens[start_idx].add_coref_label(coref_id, label_type=\"both\")\n",
    "            else:\n",
    "                conll_tokens[start_idx].add_coref_label(coref_id, label_type=\"start\")\n",
    "                conll_tokens[end_idx].add_coref_label(coref_id, label_type=\"end\")\n",
    "\n",
    "    with open(output_file_path, \"a\", encoding=\"UTF-8\") as out:\n",
    "        out.write(BEGIN)\n",
    "        for sent in sentence_list:\n",
    "            for tok in sent:\n",
    "                out.write(tok.get_conll_str() + \"\\n\")\n",
    "            out.write(SENTENCE_SEPARATOR)\n",
    "        out.write(END)\n",
    "        out.write(SENTENCE_SEPARATOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from subprocess import PIPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_conll_script(scorer_path: str, use_which_metric: str, groundtruth_file_path: str, predicted_file_path: str):\n",
    "    \"\"\"Args:\n",
    "        scorer_path: The path of the CoNLL scorer script: scorer.pl\n",
    "        use_which_metric: muc, bclub, ceafe\n",
    "        groundtruth_file_path: The path of the file serve as a ground truth file\n",
    "        predicted_file_path: The path of the file serve as a predicted output\n",
    "\n",
    "    Returns:\n",
    "        out: The standard output of the script.\n",
    "        err: The error message if the script is failed. Empty if no error.\n",
    "    \"\"\"\n",
    "    command = [scorer_path, use_which_metric, groundtruth_file_path, predicted_file_path, \"none\"]\n",
    "\n",
    "    result = subprocess.run(command, stdout=PIPE, stderr=PIPE)\n",
    "    out = result.stdout.decode(\"utf-8\")\n",
    "    err = result.stderr.decode(\"utf-8\")\n",
    "    if err:\n",
    "        err += f\" Error command: {command}\"\n",
    "    return out, err\n",
    "\n",
    "\n",
    "def resolve_conll_script_output(output_str):\n",
    "    \"\"\"Args:\n",
    "        output_str: The output of the CoNLL scorer script: scorer.pl. It only support single metric output, i.e. muc, bcub, ceafe, ceafm\n",
    "    Returns:\n",
    "        The percentage float value extracted from the script output. The ``%`` symble is omitted.\n",
    "    \"\"\"\n",
    "    regexPattern = r\"(\\d*\\.?\\d*)%\"\n",
    "    scores = [float(i) for i in re.findall(regexPattern, output_str)]\n",
    "    mention_recall = scores[0]\n",
    "    mention_precision = scores[1]\n",
    "    mention_f1 = scores[2]\n",
    "    coref_recall = scores[3]\n",
    "    coref_precision = scores[4]\n",
    "    coref_f1 = scores[5]\n",
    "    return mention_recall, mention_precision, mention_f1, coref_recall, coref_precision, coref_f1\n",
    "\n",
    "\n",
    "def compute_conll_score(conll_file_gt, conll_file_pred):\n",
    "    print(\"gt:\", conll_file_gt)\n",
    "    print(\"pred:\", conll_file_pred)\n",
    "    overall_f1 = []\n",
    "    for metric in [\"muc\", \"bcub\", \"ceafe\"]:\n",
    "        out, err = invoke_conll_script(scorer_path, metric, conll_file_gt, conll_file_pred)\n",
    "        mention_recall, mention_precision, mention_f1, coref_recall, coref_precision, coref_f1 = resolve_conll_script_output(out)\n",
    "        overall_f1.append(coref_f1)\n",
    "        print(f\"Metric: {metric}\")\n",
    "        print(f\"mention_recall, mention_precision, mention_f1: {mention_recall}, {mention_precision}, {mention_f1}\")\n",
    "        print(f\"coref_recall, coref_precision, coref_f1: {coref_recall}, {coref_precision}, {coref_f1}\")\n",
    "\n",
    "    print(f\"Overall F1: {sum(overall_f1) / len(overall_f1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt: /root/workspace/sr_coref/src/benchmarking/data/radcoref_test.conll\n",
      "pred: /root/workspace/sr_coref/src/benchmarking/data/radcoref_pred_test.conll\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Metric: muc\n",
      "mention_recall, mention_precision, mention_f1: 63.36, 76.16, 69.17\n",
      "coref_recall, coref_precision, coref_f1: 56.69, 67.65, 61.69\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Metric: bcub\n",
      "mention_recall, mention_precision, mention_f1: 63.36, 76.16, 69.17\n",
      "coref_recall, coref_precision, coref_f1: 58.73, 71.31, 64.41\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Metric: ceafe\n",
      "mention_recall, mention_precision, mention_f1: 63.36, 76.16, 69.17\n",
      "coref_recall, coref_precision, coref_f1: 62.53, 75.81, 68.53\n",
      "Overall F1: 64.87666666666667\n"
     ]
    }
   ],
   "source": [
    "compute_conll_score(conll_file_gt=test_conll_path,\n",
    "                    conll_file_pred=output_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coref_benchmark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
